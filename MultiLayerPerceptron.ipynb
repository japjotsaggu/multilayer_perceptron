{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJhL9czQGxdHu+3XTpOyXu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/japjotsaggu/multilayer_perceptron/blob/main/MultiLayerPerceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "O4XN_LxqgSTr"
      },
      "outputs": [],
      "source": [
        "#multi layer perceptron - fully connected neural netwrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "QJHFknJ8Ksh1"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron():\n",
        "  def __init__(self, position_in_layer, is_output_neuron = False):\n",
        "    self.weights = []\n",
        "    self.input = []\n",
        "    self.output = None\n",
        "\n",
        "    #backprop update\n",
        "    self.updated_weights = []\n",
        "    #how to update weights\n",
        "    self.is_output_neuron = is_output_neuron\n",
        "    #delta for update at backpropogation\n",
        "    self.delta = None\n",
        "    #used for updating in backprop\n",
        "    self.position_in_layer = position_in_layer\n",
        "\n",
        "  def attach_to_output(self, neurons):\n",
        "    '''\n",
        "       Helper function to store the reference of the other neurons\n",
        "       to this particular neuron (used for backpropagation)\n",
        "    '''\n",
        "    self.output_neurons = neurons\n",
        "\n",
        "  def sigmoid(self, x):\n",
        "    '''\n",
        "       sigmoid function for activation\n",
        "    '''\n",
        "    return 1/ (1 + math.exp(-x))\n",
        "\n",
        "  def init_weights(self, num_input):\n",
        "    '''\n",
        "       when we know the num of inputs for a neuron, this is to setup weights\n",
        "    '''\n",
        "    #randomly initialize the weights\n",
        "    for i in range(num_input+1):\n",
        "      self.weights.append(random.uniform(0, 1))\n",
        "\n",
        "  def predict(self, row):\n",
        "    #reset the inputs\n",
        "    self.inputs = []\n",
        "    #iterating over the weights and the feautres in a given row\n",
        "    activation = 0\n",
        "    for weight, feature in zip(self.weights, row):\n",
        "      self.inputs.append(feature)\n",
        "      activation = activation + weight*feature\n",
        "\n",
        "    self.output = self.sigmoid(activation)\n",
        "    return self.output\n",
        "\n",
        "\n",
        "  def update_neuron(self):\n",
        "    '''\n",
        "       updating weights at the end of backpropogation\n",
        "    '''\n",
        "    self.weights = []\n",
        "    for new_weight in self.updated_weights:\n",
        "      self.weights.append(new_weight)\n",
        "\n",
        "  def calculate_update(self, learning_rate, target):\n",
        "    '''\n",
        "       Calculating updated weights for this neuron.\n",
        "       First calculate the right delta, then calculate the right updated weights.\n",
        "       Not overwriting the weights as yet as they are needed for other update in backprop algo\n",
        "    '''\n",
        "    if self.is_output_neuron:\n",
        "      #calculating delta\n",
        "      self.delta = (self.output - target)*self.output*(1-self.output)\n",
        "\n",
        "    else:\n",
        "      delta_sum = 0\n",
        "      #to know which weights this neuron is contributing in the output layer\n",
        "      curr_weight_index = self.position_in_layer\n",
        "      for output_neuron in self.output_neurons:\n",
        "        delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[curr_weight_index])\n",
        "\n",
        "      #update this neuron delta\n",
        "      self.delta = delta_sum*self.output*(1-self.output)\n",
        "\n",
        "\n",
        "    self.updated_weights = []\n",
        "\n",
        "    #iterate over each weight and update\n",
        "    for curr_weight, curr_input in zip(self.weights, self.inputs):\n",
        "      gradient = self.delta*curr_input\n",
        "      new_weight = curr_weight - learning_rate*gradient\n",
        "      self.updated_weights.append(new_weight)\n",
        "\n"
      ],
      "metadata": {
        "id": "WhYZ3jmqmrVH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Layer():\n",
        "  '''\n",
        "     a layer in a fully connected feed forward neural network\n",
        "  '''\n",
        "\n",
        "  def __init__(self, num_neuron, is_output_layer = False):\n",
        "    self.is_output_layer = is_output_layer\n",
        "    self.neurons = []\n",
        "    #creating neurons\n",
        "    for i in range(num_neuron):\n",
        "      neuron = Neuron(i, is_output_neuron = is_output_layer)\n",
        "      self.neurons.append(neuron)\n",
        "\n",
        "  def attach(self, layer):\n",
        "    '''\n",
        "        attaching neurons from this layer to another one\n",
        "        needed for backrprop algorithm\n",
        "    '''\n",
        "    for in_neuron in self.neurons:\n",
        "      in_neuron.attach_to_output(layer.neurons)\n",
        "\n",
        "  def init_layer(self, num_input):\n",
        "    '''\n",
        "       initializing the weights of each neuron in the layer\n",
        "    '''\n",
        "    for neuron in self.neurons:\n",
        "      neuron.init_weights(num_input)\n",
        "\n",
        "  def predict(self, row):\n",
        "    '''\n",
        "       calculating activation for the full layer given the row of data streaming in\n",
        "    '''\n",
        "    row.append(1) #bias\n",
        "    activations = [neuron.predict(row) for neuron in self.neurons]\n",
        "    return activations"
      ],
      "metadata": {
        "id": "C7InUyRXJlMm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron():\n",
        "  '''\n",
        "     an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
        "  '''\n",
        "  def __init__(self, learning_rate = 0.01, num_iterations = 100):\n",
        "    self.layers = []\n",
        "    self.learning_rate = learning_rate\n",
        "    self.num_iterations = num_iterations\n",
        "\n",
        "  def add_output_layer(self, num_neuron):\n",
        "    '''\n",
        "       creating a new output layer and adding it to the architecture\n",
        "    '''\n",
        "    self.layers.insert(0, Layer(num_neuron, is_output_layer = True))\n",
        "\n",
        "  def add_hidden_layer(self, num_neuron):\n",
        "    '''\n",
        "      create a hidden layer, add it to the architecture and finally attach it to the front of the architecture\n",
        "    '''\n",
        "    hidden_layer = Layer(num_neuron)\n",
        "    hidden_layer.attach(self.layers[0])\n",
        "    self.layers.insert(0, hidden_layer)\n",
        "\n",
        "  def update_layers(self, target):\n",
        "    '''\n",
        "        calculating updated weights & then updating the weights all at once\n",
        "    '''\n",
        "    for layer in reversed(self.layers):\n",
        "      for neuron in layer.neurons:\n",
        "        neuron.calculate_update(self.learning_rate, target)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      for neuron in layer.neurons:\n",
        "        neuron.update_neuron()\n",
        "\n",
        "  def fit(self, X, y):\n",
        "    '''\n",
        "        main training function of nn algorithm.\n",
        "        will make use of backprop\n",
        "        stochastic gradient descent by selecting one row at random from the dataset\n",
        "        and use predict to calculate the error\n",
        "        the error is then backpropogated and new weights calculated\n",
        "        whole network weights updated\n",
        "    '''\n",
        "    num_row = len(X)\n",
        "    num_feature = len(X[0])\n",
        "\n",
        "    #init the weights throughout each of the layer\n",
        "    self.layers[0].init_layer(num_feature)\n",
        "\n",
        "    for i in range(1, len(self.layers)):\n",
        "      num_input = len(self.layers[i-1].neurons)\n",
        "      self.layers[i].init_layer(num_input)\n",
        "\n",
        "    for i in range(self.num_iterations):\n",
        "      r_i = random.randint(0, num_row -1)\n",
        "      row = X[r_i]\n",
        "      yhat = self.predict(row)\n",
        "      target = y[r_i]\n",
        "\n",
        "      self.update_layers(target)\n",
        "\n",
        "      #calculating error at every 100 iterations\n",
        "      if i%1000 == 0:\n",
        "        total_error = 0\n",
        "        for r_i in range(num_row):\n",
        "          row = X[r_i]\n",
        "          yhat = self.predict(row)\n",
        "          error = (y[r_i] - yhat)\n",
        "          total_error = total_error + error**2\n",
        "        mean_error = total_error/num_row\n",
        "\n",
        "  def predict(self, row):\n",
        "    '''\n",
        "        takes a row of input and returns the output of the whole neural network\n",
        "    '''\n",
        "    activations = self.layers[0].predict(row)\n",
        "    for i in range(1, len(self.layers)):\n",
        "      activations = self.layers[i].predict(activations)\n",
        "\n",
        "    outputs = []\n",
        "    for activation in activations:\n",
        "      if activation >= 0.5:\n",
        "        outputs.append(1.0)\n",
        "\n",
        "      else:\n",
        "        outputs.append(0.0)\n",
        "\n",
        "    return outputs[0]\n"
      ],
      "metadata": {
        "id": "DPTJUIdXJvj_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# XOR function (to test the nn)\n",
        "X = [[0,0], [0,1], [1,0], [1,1]]\n",
        "y = [0, 1, 1, 0]\n",
        "\n",
        "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iterations = 100000)\n",
        "# Create the architecture\n",
        "clf.add_output_layer(num_neuron = 1)\n",
        "clf.add_hidden_layer(num_neuron = 3)\n",
        "clf.add_hidden_layer(num_neuron = 2)\n",
        "\n",
        "\n",
        "clf.fit(X,y)"
      ],
      "metadata": {
        "id": "-5qp5uHLLje3"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict([0,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nJ6zd8_L3FH",
        "outputId": "b30c8ae6-64b3-46df-bfdf-3f53f2602af6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf.predict([1,0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu5N2dT5NJ15",
        "outputId": "93984954-6d9b-4971-b003-e927624ea077"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gTf4IiVgNNCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}