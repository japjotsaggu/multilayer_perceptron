# Multi-layer Perceptron

A multi-layer perceptron is an artificial, feedforward neural network - meaning that the information moves through the network from the input layer, through the hidden layers, and finally to the output layer. 

## What's inside
The purpose of this repo is to provide an implementation of the backpropagation algorithm from the ground up and understand how it fits into the functioning of the neural network. 
We use the XOR to test the functioning of the neural network and the role of backprop in capturing more complex non-linear relationship between inputs and outputs.

## Backpropagation 
An in-depth exploration of backprop and its underlying mathematical principles is carried out in one of my Medium posts [here](https://medium.com/@japjotsaggu31/backpropagation-basics-busted-44f1cbfa8308).
In combination with the 'from-scratch' implementation provided here, this material is meant to provide conceptual, practical and mathematical understanding of the alleged blackbox of neural network - backpropagation.

Note: In the notebook, a simple random weight initialization startegy is used since our goal is to understand backpropogation.
